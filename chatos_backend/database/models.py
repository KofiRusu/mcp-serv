"""
models.py - SQLAlchemy models for the learning loop database.

Defines the schema for:
- Data sources (HuggingFace, web scraping, internal)
- Training examples with metadata
- Knowledge domains for gap analysis
- Active learning tasks and queues
- Web scraping targets and results
"""

import json
from datetime import datetime
from enum import Enum as PyEnum
from typing import Any, Dict, List, Optional

from sqlalchemy import (
    Boolean,
    Column,
    DateTime,
    Enum,
    Float,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
    UniqueConstraint,
    func,
)
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.types import TypeDecorator

Base = declarative_base()


# =============================================================================
# Custom Types
# =============================================================================

class JSONType(TypeDecorator):
    """
    Platform-agnostic JSON type.
    
    Uses JSONB on PostgreSQL, TEXT with JSON serialization elsewhere.
    """
    impl = Text
    cache_ok = True

    def load_dialect_impl(self, dialect):
        if dialect.name == "postgresql":
            return dialect.type_descriptor(JSONB())
        return dialect.type_descriptor(Text())

    def process_bind_param(self, value, dialect):
        if value is not None and dialect.name != "postgresql":
            return json.dumps(value)
        return value

    def process_result_value(self, value, dialect):
        if value is not None and dialect.name != "postgresql":
            return json.loads(value)
        return value


# =============================================================================
# Enums
# =============================================================================

class SourceType(str, PyEnum):
    """Types of data sources."""
    INTERNAL = "internal"           # ChatOS conversations, PersRM data
    HUGGINGFACE = "huggingface"     # HuggingFace datasets
    WEB_SCRAPE = "web_scrape"       # Web scraped content
    SYNTHETIC = "synthetic"         # Generated by Ollama
    MANUAL = "manual"               # Manually curated


class ExampleStatus(str, PyEnum):
    """Status of training examples."""
    PENDING = "pending"             # Awaiting processing
    PROCESSED = "processed"         # Ready for training
    FLAGGED = "flagged"             # Needs review
    EXCLUDED = "excluded"           # Excluded from training
    USED = "used"                   # Already used in a training run


class DifficultyLevel(str, PyEnum):
    """Difficulty levels for curriculum learning."""
    BASIC = "basic"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"
    EXPERT = "expert"


class TaskType(str, PyEnum):
    """Types of active learning tasks."""
    GAP_FILL = "gap_fill"           # Fill knowledge gap
    UNCERTAINTY = "uncertainty"      # Clarify uncertain areas
    AUGMENT = "augment"             # Augment existing data
    VALIDATE = "validate"           # Validate generated data


class TaskStatus(str, PyEnum):
    """Status of active learning tasks."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


# =============================================================================
# Models
# =============================================================================

class DataSource(Base):
    """
    Registry of data sources for training.
    
    Tracks HuggingFace datasets, web scraping targets, and internal data.
    """
    __tablename__ = "data_sources"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(255), unique=True, nullable=False)
    source_type = Column(Enum(SourceType), nullable=False)
    description = Column(Text)
    
    # Configuration (dataset name, URL, path, etc.)
    config = Column(JSONType, default=dict)
    
    # Statistics
    total_examples = Column(Integer, default=0)
    processed_examples = Column(Integer, default=0)
    last_sync_at = Column(DateTime)
    
    # Status
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    examples = relationship("TrainingExample", back_populates="source")

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "name": self.name,
            "source_type": self.source_type.value if self.source_type else None,
            "description": self.description,
            "config": self.config,
            "total_examples": self.total_examples,
            "processed_examples": self.processed_examples,
            "last_sync_at": self.last_sync_at.isoformat() if self.last_sync_at else None,
            "is_active": self.is_active,
        }


class KnowledgeDomain(Base):
    """
    Knowledge domain taxonomy for gap analysis.
    
    Used to track coverage across different topic areas.
    """
    __tablename__ = "knowledge_domains"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(100), unique=True, nullable=False)
    category = Column(String(100))  # e.g., "programming", "ui_ux", "general"
    description = Column(Text)
    parent_id = Column(Integer, ForeignKey("knowledge_domains.id"))
    
    # Coverage thresholds
    min_examples_required = Column(Integer, default=50)
    target_examples = Column(Integer, default=200)
    
    created_at = Column(DateTime, default=datetime.utcnow)

    # Self-referential relationship for hierarchical domains
    parent = relationship("KnowledgeDomain", remote_side=[id], backref="children")
    
    # Relationships
    examples = relationship("TrainingExample", back_populates="domain")
    coverage_analyses = relationship("CoverageAnalysis", back_populates="domain")

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "name": self.name,
            "category": self.category,
            "description": self.description,
            "parent_id": self.parent_id,
            "min_examples_required": self.min_examples_required,
            "target_examples": self.target_examples,
        }


class TrainingExample(Base):
    """
    Unified storage for all training examples.
    
    Stores examples from all sources in a common format.
    """
    __tablename__ = "training_examples"

    id = Column(Integer, primary_key=True, autoincrement=True)
    
    # Source tracking
    source_id = Column(Integer, ForeignKey("data_sources.id"))
    external_id = Column(String(255))  # ID from source (HF dataset row, URL hash, etc.)
    
    # Content (chat format)
    system_prompt = Column(Text)
    user_input = Column(Text, nullable=False)
    assistant_output = Column(Text, nullable=False)
    
    # Full messages array for multi-turn conversations
    messages = Column(JSONType)
    
    # Classification
    domain_id = Column(Integer, ForeignKey("knowledge_domains.id"))
    difficulty = Column(Enum(DifficultyLevel), default=DifficultyLevel.INTERMEDIATE)
    
    # Quality metrics
    quality_score = Column(Float)  # 0.0 to 1.0
    confidence_score = Column(Float)  # Model confidence (for active learning)
    user_rating = Column(Integer)  # User feedback (-1, 0, 1)
    
    # Status
    status = Column(Enum(ExampleStatus), default=ExampleStatus.PENDING)
    
    # Extra data
    extra_data = Column(JSONType, default=dict)
    content_hash = Column(String(64), index=True)  # For deduplication
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    processed_at = Column(DateTime)
    used_at = Column(DateTime)  # When used in training

    # Relationships
    source = relationship("DataSource", back_populates="examples")
    domain = relationship("KnowledgeDomain", back_populates="examples")

    __table_args__ = (
        Index("ix_training_examples_source_external", "source_id", "external_id"),
        Index("ix_training_examples_domain_status", "domain_id", "status"),
        Index("ix_training_examples_quality", "quality_score"),
        UniqueConstraint("source_id", "external_id", name="uq_source_external_id"),
    )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "source_id": self.source_id,
            "external_id": self.external_id,
            "system_prompt": self.system_prompt,
            "user_input": self.user_input,
            "assistant_output": self.assistant_output,
            "messages": self.messages,
            "domain_id": self.domain_id,
            "difficulty": self.difficulty.value if self.difficulty else None,
            "quality_score": self.quality_score,
            "confidence_score": self.confidence_score,
            "user_rating": self.user_rating,
            "status": self.status.value if self.status else None,
            "extra_data": self.extra_data,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }

    def to_training_format(self) -> Dict[str, Any]:
        """Convert to Unsloth training format."""
        if self.messages:
            return {"messages": self.messages}
        
        messages = []
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})
        messages.append({"role": "user", "content": self.user_input})
        messages.append({"role": "assistant", "content": self.assistant_output})
        
        return {"messages": messages}


class CoverageAnalysis(Base):
    """
    Track coverage analysis for knowledge domains.
    
    Used by the active learning engine to identify gaps.
    """
    __tablename__ = "coverage_analysis"

    id = Column(Integer, primary_key=True, autoincrement=True)
    domain_id = Column(Integer, ForeignKey("knowledge_domains.id"), nullable=False)
    
    # Coverage metrics
    example_count = Column(Integer, default=0)
    avg_quality_score = Column(Float)
    avg_confidence_score = Column(Float)
    difficulty_distribution = Column(JSONType)  # {"basic": 10, "intermediate": 20, ...}
    
    # Gap analysis
    coverage_ratio = Column(Float)  # example_count / target_examples
    is_gap = Column(Boolean, default=False)  # Below min_examples_required
    priority_score = Column(Float)  # Higher = needs more data
    
    # Recommendations
    recommended_sources = Column(JSONType)  # Suggested sources to fill gap
    recommended_actions = Column(JSONType)  # e.g., ["generate_synthetic", "scrape_docs"]
    
    analyzed_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    domain = relationship("KnowledgeDomain", back_populates="coverage_analyses")

    __table_args__ = (
        Index("ix_coverage_analysis_gap", "is_gap", "priority_score"),
    )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "domain_id": self.domain_id,
            "domain_name": self.domain.name if self.domain else None,
            "example_count": self.example_count,
            "avg_quality_score": self.avg_quality_score,
            "coverage_ratio": self.coverage_ratio,
            "is_gap": self.is_gap,
            "priority_score": self.priority_score,
            "recommended_actions": self.recommended_actions,
            "analyzed_at": self.analyzed_at.isoformat() if self.analyzed_at else None,
        }


class ScrapeTarget(Base):
    """
    Web scraping targets configuration.
    
    Defines URLs and patterns for scraping training data.
    """
    __tablename__ = "scrape_targets"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(255), nullable=False)
    url_pattern = Column(String(1024), nullable=False)  # URL or pattern
    
    # Scraping configuration
    scrape_type = Column(String(50))  # "documentation", "qa", "tutorial", "code"
    selector_config = Column(JSONType)  # CSS/XPath selectors
    rate_limit_seconds = Column(Float, default=1.0)
    max_pages = Column(Integer, default=100)
    
    # Target domain for extracted content
    target_domain_id = Column(Integer, ForeignKey("knowledge_domains.id"))
    
    # Status
    is_active = Column(Boolean, default=True)
    last_scraped_at = Column(DateTime)
    total_pages_scraped = Column(Integer, default=0)
    total_examples_extracted = Column(Integer, default=0)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    target_domain = relationship("KnowledgeDomain")
    results = relationship("ScrapeResult", back_populates="target")

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "name": self.name,
            "url_pattern": self.url_pattern,
            "scrape_type": self.scrape_type,
            "is_active": self.is_active,
            "last_scraped_at": self.last_scraped_at.isoformat() if self.last_scraped_at else None,
            "total_pages_scraped": self.total_pages_scraped,
            "total_examples_extracted": self.total_examples_extracted,
        }


class ScrapeResult(Base):
    """
    Raw results from web scraping.
    
    Stores scraped content before processing into training examples.
    """
    __tablename__ = "scrape_results"

    id = Column(Integer, primary_key=True, autoincrement=True)
    target_id = Column(Integer, ForeignKey("scrape_targets.id"), nullable=False)
    
    url = Column(String(2048), nullable=False)
    url_hash = Column(String(64), index=True)  # For deduplication
    
    # Raw content
    title = Column(Text)
    raw_content = Column(Text)
    extracted_content = Column(JSONType)  # Structured extracted data
    
    # Processing status
    is_processed = Column(Boolean, default=False)
    examples_generated = Column(Integer, default=0)
    
    # Metadata
    http_status = Column(Integer)
    content_type = Column(String(100))
    scraped_at = Column(DateTime, default=datetime.utcnow)
    processed_at = Column(DateTime)
    error_message = Column(Text)

    # Relationships
    target = relationship("ScrapeTarget", back_populates="results")

    __table_args__ = (
        Index("ix_scrape_results_target_processed", "target_id", "is_processed"),
    )


class ActiveLearningTask(Base):
    """
    Active learning task queue.
    
    Tracks tasks for filling knowledge gaps and improving coverage.
    """
    __tablename__ = "active_learning_tasks"

    id = Column(Integer, primary_key=True, autoincrement=True)
    
    task_type = Column(Enum(TaskType), nullable=False)
    status = Column(Enum(TaskStatus), default=TaskStatus.PENDING)
    priority = Column(Integer, default=0)  # Higher = more important
    
    # Task configuration
    target_domain_id = Column(Integer, ForeignKey("knowledge_domains.id"))
    target_count = Column(Integer, default=10)  # Examples to generate/find
    config = Column(JSONType)  # Task-specific configuration
    
    # Progress
    examples_generated = Column(Integer, default=0)
    examples_validated = Column(Integer, default=0)
    
    # Results
    result = Column(JSONType)
    error_message = Column(Text)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime)
    completed_at = Column(DateTime)

    # Relationships
    target_domain = relationship("KnowledgeDomain")

    __table_args__ = (
        Index("ix_active_learning_tasks_status_priority", "status", "priority"),
    )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "task_type": self.task_type.value if self.task_type else None,
            "status": self.status.value if self.status else None,
            "priority": self.priority,
            "target_domain_id": self.target_domain_id,
            "target_count": self.target_count,
            "examples_generated": self.examples_generated,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }


class TrainingRun(Base):
    """
    Record of training runs using the learning loop data.
    
    Links training jobs to the examples used.
    """
    __tablename__ = "training_runs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    
    # Link to existing job store
    job_id = Column(String(100), unique=True, nullable=False)
    
    # Dataset info
    dataset_version = Column(Integer)
    total_examples = Column(Integer)
    example_ids = Column(JSONType)  # List of TrainingExample IDs used
    
    # Domain distribution
    domain_distribution = Column(JSONType)  # {"python": 100, "ui_components": 50, ...}
    difficulty_distribution = Column(JSONType)
    source_distribution = Column(JSONType)
    
    # Quality metrics
    avg_quality_score = Column(Float)
    
    # Results
    final_loss = Column(Float)
    eval_metrics = Column(JSONType)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "job_id": self.job_id,
            "dataset_version": self.dataset_version,
            "total_examples": self.total_examples,
            "domain_distribution": self.domain_distribution,
            "avg_quality_score": self.avg_quality_score,
            "final_loss": self.final_loss,
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }


# =============================================================================
# Import Notes Models (for create_all to pick them up)
# =============================================================================

from chatos_backend.database.notes_models import NoteDB, TranscriptDB

__all__ = [
    "Base",
    "JSONType",
    "SourceType",
    "ExampleStatus",
    "DifficultyLevel",
    "TaskType",
    "TaskStatus",
    "DataSource",
    "KnowledgeDomain",
    "TrainingExample",
    "CoverageAnalysis",
    "ScrapeTarget",
    "ScrapeResult",
    "ActiveLearningTask",
    "TrainingRun",
    "NoteDB",
    "TranscriptDB",
]

